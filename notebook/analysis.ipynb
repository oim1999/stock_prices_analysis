{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANALYSIS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing OS and sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "nbook_dir = os.getcwd()\n",
    "\n",
    "# Add the parent directory of the 'scripts' folder to sys.path\n",
    "sys.path.append(os.path.abspath(os.path.join(nbook_dir, '..')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### importing my functions to use modularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "from scripts.data_loader import load_data\n",
    "from scripts.data_loader import load_data_finance\n",
    "from scripts.data_analysis import get_sentiment\n",
    "from scripts.data_analysis import classify_sentiment\n",
    "from scripts.data_analysis import calc_moving_avrg\n",
    "from scripts.data_analysis import parse_mixed_dates\n",
    "from scripts.data_visualization import plot_data\n",
    "from scripts.data_visualization import plot_univariate\n",
    "from scripts.data_visualization import plot_bivariate\n",
    "from scripts.data_visualization import plot_top_publishers\n",
    "from scripts.data_visualization import plot_top_keywords\n",
    "from scripts.data_visualization import plot_publication_frequency\n",
    "from scripts.data_visualization import plot_publishing_times\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data\n",
    "data_yfinance = load_data_finance(\"../yfinance_data/AAPL_historical_data.csv\")\n",
    "data = load_data(\"../raw_analyst_ratings.csv/raw_analyst_ratings.csv\")\n",
    "\n",
    "#print(\"Dataset Overview:\")\n",
    "#print(data.head())\n",
    "#data.head()\n",
    "print(data.columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis on Headlines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Add a 'sentiment' column with sentiment scores\n",
    "data['sentiment'] = data['headline'].apply(get_sentiment)\n",
    "\n",
    "data['sentiment_category'] = data['sentiment'].apply(classify_sentiment)\n",
    "\n",
    "# Group by Stock Symbol to get average sentiment per stock\n",
    "stock_sentiment = data.groupby('stock')['sentiment'].mean().reset_index()\n",
    "stock_sentiment = stock_sentiment.rename(columns={'sentiment': 'average_sentiment'})\n",
    "\n",
    "# Display Results\n",
    "print(\"\\nSentiment Analysis Results:\")\n",
    "print(data[['headline', 'stock', 'sentiment', 'sentiment_category']].head())\n",
    "\n",
    "print(\"\\nAverage Sentiment per Stock:\")\n",
    "print(stock_sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count number of articles per publisher\n",
    "publisher_counts = data['publisher'].value_counts()\n",
    "\n",
    "print(\"Number of Articles Per Publisher:\")\n",
    "print(publisher_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Apply the function to the 'date' column\n",
    "data['date'] = data['date'].apply(parse_mixed_dates)\n",
    "\n",
    "# Extract day, month, and year after fixing the column\n",
    "data['day_of_week'] = data['date'].dt.day_name()\n",
    "data['month'] = data['date'].dt.month_name()\n",
    "data['year'] = data['date'].dt.year\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract Common Keywords/Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tokenize and vectorize the 'headline' column\n",
    "vectorizer = CountVectorizer(stop_words='english', max_features=20)\n",
    "word_matrix = vectorizer.fit_transform(data['headline'])\n",
    "word_counts = pd.DataFrame(word_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "# Sum word frequencies\n",
    "word_frequencies = word_counts.sum(axis=0).sort_values(ascending=False)\n",
    "print(\"Top Keywords in Headlines:\")\n",
    "print(word_frequencies)\n",
    "\n",
    "plot_top_keywords(word_frequencies)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Topic Modeling with LDA (Latent Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_features=1000)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['headline'])\n",
    "\n",
    "# Fit LDA model\n",
    "lda = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics\n",
    "lda.fit(tfidf_matrix)\n",
    "\n",
    "# Display the top words for each topic\n",
    "words = tfidf_vectorizer.get_feature_names_out()\n",
    "for i, topic in enumerate(lda.components_):\n",
    "    print(f\"Topic {i+1}: \", [words[i] for i in topic.argsort()[-10:]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Publication Frequency Over Time\n",
    "#### Group and Count Articles by Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'date' column to datetime if not already\n",
    "data['date'] = pd.to_datetime(data['date'], errors='coerce', utc=True)\n",
    "data = data.dropna(subset=['date'])\n",
    "\n",
    "\n",
    "# Group by date and count articles\n",
    "daily_publications = data.groupby(data['date'].dt.date).size()\n",
    "plot_publication_frequency(daily_publications)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Spikes in Publication Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Summary Statistics for Daily Publications:\")\n",
    "print(daily_publications.describe())\n",
    "\n",
    "# Identify spikes (e.g., days with publication counts above the 95th percentile)\n",
    "threshold = daily_publications.quantile(0.95)\n",
    "spikes = daily_publications[daily_publications > threshold]\n",
    "print(\"\\nSpikes in Publication Frequency:\")\n",
    "print(spikes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Publishing Times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hour'] = data['date'].dt.hour\n",
    "hourly_publications = data['hour'].value_counts().sort_index()\n",
    "plot_publishing_times(hourly_publications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a moving average column\n",
    "data_yfinance[\"Moving Average\"] = calc_moving_avrg(data_yfinance[\"Close\"], window=5)\n",
    "\n",
    "plot_data(data_yfinance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### univariate analysis takes the data_yfinance and the column \"close\" and plots the distribution\n",
    "plot_univariate(data_yfinance, 'Close', title=\"Closing price distribution\")\n",
    "\n",
    "### bivariate analysis takes the data_yfinance and the columns \"close\" and \"volume\"\n",
    "plot_bivariate(data_yfinance, 'Close', 'Volume', title=\"Closing price VS volume traded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
